{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d53eed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4acdc332",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = \"<s>\"\n",
    "EOS = \"</s>\"\n",
    "UNK = \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "827e0f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lines(lines):\n",
    "    pattern = re.compile(r\"([\\u0900-\\u097F]+|[।!?.,;:])\")\n",
    "    for line in lines:\n",
    "        line = str(line).strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        tokens = pattern.findall(line)\n",
    "        yield tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentence_boundaries(tokens, order=4):\n",
    "    bos = [BOS] * (order - 1)\n",
    "    return bos + tokens + [EOS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38343823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(train_tokens, min_freq=1):\n",
    "    freq = Counter(train_tokens)\n",
    "    vocab = {w for w, c in freq.items() if c >= min_freq or w in {BOS, EOS, UNK}}\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_unk(tokens, vocab):\n",
    "    return [t if t in vocab else UNK for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cab8c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(tokens, n):\n",
    "    for i in range(n - 1, len(tokens)):\n",
    "        yield tuple(tokens[i - n + 1 : i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramLM4DeletedInterp:\n",
    "    def __init__(self):\n",
    "        self.order = 4\n",
    "        self.counts = {1: Counter(), 2: Counter(), 3: Counter(), 4: Counter()}\n",
    "        self.context_counts = {1: 0, 2: Counter(), 3: Counter(), 4: Counter()}\n",
    "        self.vocab = set()\n",
    "        self.N_tokens = 0\n",
    "        self.lambdas = [0.25, 0.25, 0.25, 0.25]\n",
    "\n",
    "    def fit(self, sentences, min_freq=1):\n",
    "        flat = []\n",
    "        for sent in sentences:\n",
    "            s = add_sentence_boundaries(sent, order=self.order)\n",
    "            flat.extend(s)\n",
    "        self.N_tokens = len(flat)\n",
    "        self.vocab = build_vocab(flat, min_freq=min_freq)\n",
    "        flat = map_to_unk(flat, self.vocab)\n",
    "\n",
    "        for n in [1, 2, 3, 4]:\n",
    "            self.counts[n].update(ngrams(flat, n))\n",
    "\n",
    "        self.context_counts[1] = self.N_tokens\n",
    "        for n in [2, 3, 4]:\n",
    "            ctx_counter = Counter()\n",
    "            for ng, c in self.counts[n].items():\n",
    "                ctx = ng[:-1]\n",
    "                ctx_counter[ctx] += c\n",
    "            self.context_counts[n] = ctx_counter\n",
    "\n",
    "    def _p_ml(self, w, context):\n",
    "        k = len(context) + 1\n",
    "        if k == 1:\n",
    "            return self.counts[1][(w,)] / max(1, self.context_counts[1])\n",
    "        elif k == 2:\n",
    "            return self.counts[2][context + (w,)] / self.context_counts[2][context] if self.context_counts[2][context] > 0 else 0.0\n",
    "        elif k == 3:\n",
    "            return self.counts[3][context + (w,)] / self.context_counts[3][context] if self.context_counts[3][context] > 0 else 0.0\n",
    "        elif k == 4:\n",
    "            return self.counts[4][context + (w,)] / self.context_counts[4][context] if self.context_counts[4][context] > 0 else 0.0\n",
    "\n",
    "    def prob(self, w, context3):\n",
    "        w = w if w in self.vocab else UNK\n",
    "        c3, c2, c1 = context3, context3[1:], context3[2:]\n",
    "        p4, p3, p2, p1 = self._p_ml(w, c3), self._p_ml(w, c2), self._p_ml(w, c1), self._p_ml(w, ())\n",
    "        λ1, λ2, λ3, λ4 = self.lambdas\n",
    "        return λ4*p4 + λ3*p3 + λ2*p2 + λ1*p1\n",
    "\n",
    "    def learn_lambdas_deleted_interpolation(self):\n",
    "        assign_counts = [0, 0, 0, 0]\n",
    "        for quad, c in self.counts[4].items():\n",
    "            w1, w2, w3, w4 = quad\n",
    "            c4, d4 = c - 1, self.context_counts[4][quad[:-1]] - 1\n",
    "            p4 = (c4/d4) if d4 > 0 and c4 > 0 else 0.0\n",
    "\n",
    "            c3, d3 = self.counts[3][(w2, w3, w4)] - 1, self.context_counts[3][(w2, w3)] - 1\n",
    "            p3 = (c3/d3) if d3 > 0 and c3 > 0 else 0.0\n",
    "\n",
    "            c2, d2 = self.counts[2][(w3, w4)] - 1, self.context_counts[2][(w3,)] - 1\n",
    "            p2 = (c2/d2) if d2 > 0 and c2 > 0 else 0.0\n",
    "\n",
    "            c1, d1 = self.counts[1][(w4,)] - 1, self.context_counts[1] - 1\n",
    "            p1 = (c1/d1) if d1 > 0 and c1 > 0 else 0.0\n",
    "\n",
    "            probs = [p1, p2, p3, p4]\n",
    "            k = probs.index(max(probs))\n",
    "            assign_counts[k] += c\n",
    "\n",
    "        total = sum(assign_counts)\n",
    "        self.lambdas = [cnt / total for cnt in assign_counts] if total > 0 else [0.25]*4\n",
    "\n",
    "    def sent_logprob(self, sent_tokens):\n",
    "        s = add_sentence_boundaries(sent_tokens, order=self.order)\n",
    "        s = map_to_unk(s, self.vocab)\n",
    "        logp = 0.0\n",
    "        for i in range(3, len(s)):\n",
    "            w, ctx = s[i], (s[i-3], s[i-2], s[i-1])\n",
    "            p = self.prob(w, ctx) or 1e-12\n",
    "            logp += math.log(p)\n",
    "        return logp\n",
    "\n",
    "    def corpus_perplexity(self, sentences):\n",
    "        total_logp, total_tokens = 0.0, 0\n",
    "        for sent in sentences:\n",
    "            total_logp += self.sent_logprob(sent)\n",
    "            total_tokens += len(sent) + 1\n",
    "        return math.exp(-total_logp / max(1, total_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b833836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_dev(data, dev_ratio=0.1, seed=42):\n",
    "    random.Random(seed).shuffle(data)\n",
    "    k = int(len(data) * (1 - dev_ratio))\n",
    "    return data[:k], data[k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46756e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned λ (uni, bi, tri, quad): [0.0019316206297083252, 0.25671238168823646, 0.27583542592234883, 0.4655205717597064]\n",
      "Train PPL: 2.8126192310489193\n",
      "Dev PPL: 2.8973247418014534\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"train.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# Extract sentences (plain text)\n",
    "sentences = df[\"sentence\"].dropna().tolist()\n",
    "\n",
    "# Tokenize (replace with your own tokenizer)\n",
    "tokenized_sents = list(tokenize_lines(sentences))\n",
    "\n",
    "# Split train/dev\n",
    "train_sents, dev_sents = split_train_dev(tokenized_sents, dev_ratio=0.1)\n",
    "\n",
    "# Train model\n",
    "model = NgramLM4DeletedInterp()\n",
    "model.fit(train_sents, min_freq=2)  # filter rare words\n",
    "model.learn_lambdas_deleted_interpolation()\n",
    "\n",
    "print(\"Learned λ (uni, bi, tri, quad):\", model.lambdas)\n",
    "print(\"Train PPL:\", model.corpus_perplexity(train_sents))\n",
    "print(\"Dev PPL:\", model.corpus_perplexity(dev_sents))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
