{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef49c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea6f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngrams(tokens, n):\n",
    "    #Return n-gram counts as Counter\n",
    "    ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    return Counter(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8f703df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_turing_smoothing(ngram_counts, vocab_size, n):\n",
    "    #Returns smoothed probabilities for given n-gram counts\n",
    "    total_seen = sum(ngram_counts.values())   # N\n",
    "    freq_of_freq = Counter(ngram_counts.values())  # Nc\n",
    "\n",
    "    N1 = freq_of_freq.get(1, 0)   # number of ngrams that occurred once\n",
    "    \n",
    "    # Total possible ngrams\n",
    "    total_possible = (vocab_size ** n)\n",
    "    num_unseen = total_possible - len(ngram_counts)\n",
    "\n",
    "    # Probability mass reserved for unseen ngrams\n",
    "    p_unseen = N1 / total_seen if total_seen > 0 else 0\n",
    "    p_each_unseen = p_unseen / num_unseen if num_unseen > 0 else 0\n",
    "\n",
    "    smoothed_probs = {}\n",
    "\n",
    "    for ng, c in ngram_counts.items():\n",
    "        Nc = freq_of_freq[c]\n",
    "        Nc1 = freq_of_freq.get(c+1, 0)\n",
    "\n",
    "        if Nc1 > 0:\n",
    "            c_star = (c+1) * (Nc1 / Nc)\n",
    "        else:\n",
    "            c_star = c  # fallback\n",
    "\n",
    "        smoothed_probs[ng] = c_star / total_seen\n",
    "\n",
    "    return smoothed_probs, p_each_unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbbad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_probability(sentence_tokens, model_probs, p_unseen, n):\n",
    "    \n",
    "    ngrams = [tuple(sentence_tokens[i:i+n]) for i in range(len(sentence_tokens)-n+1)]\n",
    "    log_prob = 0.0\n",
    "\n",
    "    for ng in ngrams:\n",
    "        if ng in model_probs:\n",
    "            log_prob += math.log(model_probs[ng])\n",
    "        else:\n",
    "            log_prob += math.log(p_unseen) if p_unseen > 0 else float(\"-inf\")\n",
    "\n",
    "    return log_prob  # log probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbb7438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "val_df   = pd.read_csv(\"validation.csv\")\n",
    "test_df  = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad3492c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(df, column=\"sentence\"):\n",
    "    \"\"\"Return list of sentences from a plain text column.\"\"\"\n",
    "    return df[column].dropna().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cbbb4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 1238 sentences\n",
      "Validation set: 154 sentences\n",
      "Test set: 156 sentences\n"
     ]
    }
   ],
   "source": [
    "train_sentences = extract_sentences(train_df)\n",
    "val_sentences   = extract_sentences(val_df)\n",
    "test_sentences  = extract_sentences(test_df)\n",
    "\n",
    "print(f\"Training set: {len(train_sentences)} sentences\")\n",
    "print(f\"Validation set: {len(val_sentences)} sentences\")\n",
    "print(f\"Test set: {len(test_sentences)} sentences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e02d98ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = [word for sent in train_sentences for word in sent.split()]\n",
    "vocab = set(train_tokens)\n",
    "V = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "625b2483",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "for n in range(1, 5):\n",
    "    counts = build_ngrams(train_tokens, n)\n",
    "    smoothed_probs, p_unseen = good_turing_smoothing(counts, V, n)\n",
    "    models[n] = (smoothed_probs, p_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f7c1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(sentences, model_probs, p_unseen, n):\n",
    "    #lower perplexity -> better model\n",
    "    total_log_prob = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        tokens = sent.split()\n",
    "        lp = sentence_probability(tokens, model_probs, p_unseen, n)\n",
    "        total_log_prob += lp\n",
    "        total_tokens += len(tokens)\n",
    "\n",
    "    if total_tokens == 0:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    return math.exp(-total_log_prob / total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f05407ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Perplexities:\n",
      "1-gram model: Perplexity = inf\n",
      "2-gram model: Perplexity = 5336695.0689\n",
      "3-gram model: Perplexity = 7465180223.2015\n",
      "4-gram model: Perplexity = 2245490105677.9380\n",
      "\n",
      "Test Perplexities:\n",
      "1-gram model: Perplexity = inf\n",
      "2-gram model: Perplexity = 6018942.6891\n",
      "3-gram model: Perplexity = 7895610246.4466\n",
      "4-gram model: Perplexity = 2293854547559.6338\n"
     ]
    }
   ],
   "source": [
    "for split_name, split_sentences in [(\"Validation\", val_sentences), (\"Test\", test_sentences)]:\n",
    "    print(f\"\\n{split_name} Perplexities:\")\n",
    "    for n in range(1, 5):\n",
    "        model_probs, p_unseen = models[n]\n",
    "        ppl = perplexity(split_sentences, model_probs, p_unseen, n)\n",
    "        print(f\"{n}-gram model: Perplexity = {ppl:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
