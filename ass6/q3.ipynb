{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14a21c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "598d48a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_dir = \"../ass4\"\n",
    "models = {\n",
    "    \"unigram\": pickle.load(open(\"unigram_model.pkl\", \"rb\")),\n",
    "    \"bigram\": pickle.load(open(os.path.join(base_dir, \"bigram_model.pkl\"), \"rb\")),\n",
    "    \"trigram\": pickle.load(open(os.path.join(base_dir, \"trigram_model.pkl\"), \"rb\")),\n",
    "    \"quadgram\": pickle.load(open(os.path.join(base_dir, \"quadgram_model.pkl\"), \"rb\")),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3307713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok2str(tok):\n",
    "    \"\"\"Convert tuple tokens or plain strings to readable strings\"\"\"\n",
    "    if isinstance(tok, tuple):\n",
    "        return \" \".join(str(t) for t in tok)\n",
    "    return str(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e72a9a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_context(model):\n",
    "    \"\"\"Pick a random valid context from the model\"\"\"\n",
    "    return random.choice(list(model.keys())) if model else ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f116bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_generate(model, n, max_len=20, end_token=\"</s>\"):\n",
    "    if n == 1:  # unigram\n",
    "        sentence = []\n",
    "        for _ in range(max_len):\n",
    "            if not model: break\n",
    "            next_word = max(model, key=model.get)\n",
    "            if tok2str(next_word) == end_token:\n",
    "                break\n",
    "            sentence.append(tok2str(next_word))\n",
    "        return \" \".join(sentence) if sentence else \"<EMPTY>\"\n",
    "\n",
    "    # start with a random context\n",
    "    context = get_random_context(model)\n",
    "    sentence = list(context)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        candidates = model.get(context, {})\n",
    "        candidates = {w: p for w, p in candidates.items() if isinstance(p, (float, int))}\n",
    "        if not candidates:\n",
    "            break\n",
    "        next_word = max(candidates, key=candidates.get)\n",
    "        if tok2str(next_word) == end_token:\n",
    "            break\n",
    "        sentence.append(next_word)\n",
    "        context = tuple(sentence[-(n - 1):])\n",
    "\n",
    "    output = [tok2str(tok) for tok in sentence[n - 1:]]\n",
    "    return \" \".join(output) if output else \"<EMPTY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5957122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_generate(model, n, beam_size=20, max_len=20, end_token=\"</s>\"):\n",
    "    if n == 1:  # unigram\n",
    "        beams = [([], 1.0)]\n",
    "        for _ in range(max_len):\n",
    "            new_beams = []\n",
    "            for seq, score in beams:\n",
    "                for word, prob in model.items():\n",
    "                    if not isinstance(prob, (float, int)):\n",
    "                        continue\n",
    "                    new_beams.append((seq + [tok2str(word)], score * prob))\n",
    "            if not new_beams:\n",
    "                return \" \".join(beams[0][0]) if beams else \"<EMPTY>\"\n",
    "            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "        return \" \".join(beams[0][0]) if beams else \"<EMPTY>\"\n",
    "\n",
    "    # start with random context\n",
    "    context = get_random_context(model)\n",
    "    beams = [(list(context), 1.0)]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        new_beams = []\n",
    "        for seq, score in beams:\n",
    "            context = tuple(seq[-(n - 1):])\n",
    "            candidates = model.get(context, {})\n",
    "            for word, prob in candidates.items():\n",
    "                if not isinstance(prob, (float, int)):\n",
    "                    continue\n",
    "                new_seq = seq + [word]\n",
    "                new_score = score * prob\n",
    "                new_beams.append((new_seq, new_score))\n",
    "        if not new_beams:\n",
    "            break\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "        if any(tok2str(seq[-1]) == end_token for seq, _ in beams):\n",
    "            break\n",
    "\n",
    "    if not beams:\n",
    "        return \"<EMPTY>\"\n",
    "\n",
    "    best_seq, _ = beams[0]\n",
    "    output = [tok2str(tok) for tok in best_seq[n - 1:]]\n",
    "    return \" \".join(output) if output else \"<EMPTY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9763649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== UNIGRAM MODEL (n=1) ===\n",
      "\n",
      "Greedy Samples:\n",
      "1:                    \n",
      "2:                    \n",
      "3:                    \n",
      "4:                    \n",
      "5:                    \n",
      "\n",
      "Beam Search Samples:\n",
      "1: \n",
      "2: \n",
      "3: \n",
      "4: \n",
      "5: \n",
      "\n",
      "=== BIGRAM MODEL (n=2) ===\n",
      "\n",
      "Greedy Samples:\n",
      "1: શકે છે અને આ કિસ્સામાં વ્યક્તિગત ઉષ્ણતા બિંદુ ( ) ની જમીન મુક્ત છે અને આ કિસ્સામાં વ્યક્તિગત ઉષ્ણતા બિંદુ\n",
      "2: સમયના વહેવા સાથે જ નહીં પરંતુ આ કિસ્સામાં વ્યક્તિગત ઉષ્ણતા બિંદુ ( ) ની જમીન મુક્ત છે અને આ કિસ્સામાં\n",
      "3: મળી હતી અને આ કિસ્સામાં વ્યક્તિગત ઉષ્ણતા બિંદુ ( ) ની જમીન મુક્ત છે અને આ કિસ્સામાં વ્યક્તિગત ઉષ્ણતા બિંદુ\n",
      "4: ટાળી દેવી યોગ્ય નથી અને આ કિસ્સામાં વ્યક્તિગત ઉષ્ણતા બિંદુ ( ) ની જમીન મુક્ત છે અને આ કિસ્સામાં વ્યક્તિગત\n",
      "5: ઘરેલુ ઉપયોગમાં લેવાય છે અને આ કિસ્સામાં વ્યક્તિગત ઉષ્ણતા બિંદુ ( ) ની જમીન મુક્ત છે અને આ કિસ્સામાં વ્યક્તિગત\n",
      "\n",
      "Beam Search Samples:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: માત્ર ઉજજડ જમીનમાં ૪૦ સોલાર સ્ટ્રીટ લાઈટ હાઉસ પ્રોજેક્ટનું જાન્યુઆરીએ ઈ ખાતમુહૂર્ત પ્રધાનમંત્રી નરેન્દ્રભાઈ મોદીના વરદ હસ્તે ટીડીઓ હિરેન ચૌહાણ\n",
      "2: ભારે ચકચાર મચી બીબીબી સંવાદદાતા નવી સેન્ટ્રોમા રિયલ વર્લ્ડમાં રજિસ્ટર્ડ ઇન્વેસ્ટમેંટ એડવાઇઝરના કાર્ય અમારી સરકારના મંત્રીઓ પદાધિકારીઓ અધિકારીઓ જેમને સ્થાયી\n",
      "3: નથી કોંગ્રેસના કાર્યકર્તાઓએ ઝડપ્યો દારૂનો જથ્થો ઝડપાયો તુલા રાશિના જાતકો ભગવાન આપણી વચ્ચેથી અણધારી વિદાય લીધી નાણામંત્રી નિર્માલ સીતારમણના શુક્રવારે\n",
      "4: આવેલ કોઠી ફળીયુ ન્યુ આશાપુરી નવાયાર્ડ જાદવપાર્ક ન્યુ આશાપુરી નવાયાર્ડ જાદવપાર્ક ન્યુ આશાપુરી નવાયાર્ડ જાદવપાર્ક ન્યુ આશાપુરી નવાયાર્ડ જાદવપાર્ક ન્યુ\n",
      "5: સેવા ઉદ્યોગો સમીક્ષાઓ ઘણો છોડી ઠેર ઠેર ઠેર ઠેર ઠેર ઠેર ઠેર ઠેર ઠેર ઠેર ઠેર ઠેર રખડવું પડે છે\n",
      "\n",
      "=== TRIGRAM MODEL (n=3) ===\n",
      "\n",
      "Greedy Samples:\n",
      "1: બની જાય છે સંદેશ ન્યૂઝની ગ્રાઉન્ડ ઝીરો રિપોર્ટ સુરતના આ વિસ્તારમાં સૌથી વધુ લોકપ્રિય કીફિર પર અનલોડ છે આ ઉપરાંત\n",
      "2: દુનિયામાં પ્રખ્યાત છે વીરપુરની ખ્યાતિનું કારણ છે તેમાં આવેલું સુપ્રસિદ્ધ જલારામ મંદિર દર વર્ષે લાખો ભક્તો અહીં જલારામ બાપાના દર્શન\n",
      "3: કવાંટમા કડકડતી ઠંડીનું મોજું ફ્રી વળતા રહીશોએ તાપણાનો સહારો લીધો હતો ભારતમાં હવા પાણી ખોરાક સૌને શુઘ્ધ મળવા જોઈએ પણ\n",
      "4: તમને પ્રભાવિત કરશે પાર્ટનર સાથે નકામી ચર્ચાથી બચશો તો સારૂ રહેશે આરોગ્ય સચિવ પહોંચ્યા શ્રેય હોસ્પિટલ સેન્ટ્રિપ્ટલ કારકિર્દીની રેખા (\n",
      "5: દૈનિક રવિવાર બંધ અત્યારે ચિત્રલેખા ઉપરાંત બીજાં પ્રકાશનમાં એમની રાજકારણને લગતી કટાર નિયમિત પ્રસિદ્ધ થાય છે અને આ રીતે જાગૃતી\n",
      "\n",
      "Beam Search Samples:\n",
      "1: વિસ્તારમાં મોટી વ્હોરવાડ કાલુપુરા રોડ પર ફત્તેપુરા વિસ્તારનું આમલી ફળીયુ ન્યુ આશાપુરી નવાયાર્ડ જાદવપાર્ક ન્યુ સમા રોડ શેરોન પાર્ક નિઝામપુરા\n",
      "2: ઠંડક પ્રસરી છે કાર વીમા પ્રદાતાઓ તમને એક નવી કાર વીમા પ્રદાતાઓ તમને એક નવી કાર વીમા પ્રદાતાઓ તમને એક\n",
      "3: સેન્ટિમેન્ટને વધુ ફટકો પડ્યો હતો તળાજા પંથકમાં પણ વરસાદ પડતા વાતાવરણમાં ઠંડક પ્રસરી છે કાર ખરીદવાનું વિચારી રહ્યા હોવ તો\n",
      "4: અને ઝડપી બનશે આ માટે કદાચ આભારી છે ભારતીય ખોરાક અને લાઈફસ્ટાઈલ અન્ય વૈશ્વિક વસ્તીની તુલનામાં ભારતીય લોકોમાં આંતરડા હેલ્ધી\n",
      "5: ફિચર્સ છે કે વાવની આસપાસ એવા કોઈ ડેમ કે નદીના વહેણ પણ નથી છતાં આ વાવમાં પાણીના ઝરા ફૂટી રહ્યા\n",
      "\n",
      "=== QUADGRAM MODEL (n=4) ===\n",
      "\n",
      "Greedy Samples:\n",
      "1: હકુમત ગદર એક પ્રેમકથા અને અપનેના અનિલ શર્મા કરશે કેવડિયા પોલીસ છાવણીમાં તબદીલ અરવલ્લીઃ બાયડમાં પેટા ચૂંટણી પહેલા કોંગ્રેસના કાર્યકર્તાઓએ\n",
      "2: માનસિક ત્રાસ અને મારકૂટ અંગેની ફરિયાદ દાખલ સ્ટોપ્સ નવા ભરતી ના સ્વાગત પર કામ કરે છે આયુષ મંત્રાલયે આ વર્ષની\n",
      "3: સરકાર ખેડૂતોને પાણી નહી આપે તો ખેડૂતો હિંસક બનશે અને સરકારી મિલકતોને નુકશાન પહોંચાડશે તો સમગ્ર જવાબદારી તંત્રની રહેશે ૧૫\n",
      "4: સાંજે તમે સામાજિક કાર્ય અને આનંદમાં વ્યસ્ત રહેશો ભાગ્ય તમને ટકા સુધી સાથ આપી રહ્યું છે મહિલા અદાલતની કામકાજની ભાષા\n",
      "5: કર્યું ત્યારે એક દિવસ ભાયાતો સાથે શિકાર ખેલતી વખતે આજનું જે હળવદ છે ત્યાં રાજોધરજી ના ઘોડાની સામે એક સસલુ\n",
      "\n",
      "Beam Search Samples:\n",
      "1: અન્ય જિલ્લામાં જવા પર પ્રતિબંધ રહેશે આ સમયે ફક્ત ઈમરજન્સી અને જરૂરી સેવાને ડ્રાઈવરની સાથે ટકા બેઠક ક્ષમતાની પરમિશન મળશે\n",
      "2: પણ ટ્રેનિંગ આપવામાં આવશે વેક્સિનેશન ઝડપી થાય અને યોગ્ય પ્રકારે થાય તે માટે પાંચ વ્યકિતની એમ ૫૦૦ લોકોની ટીમ તૈયાર\n",
      "3: પણ આપવામાં આવ્યું છે ઉલ્લેખનીય છે કે લોકસભા ચૂંટણીમાં કોંગ્રેસના કારમા પરાજયની જવાબદારી સ્વીકારીને રાહુલ ગાંધીએ પાર્ટીના અધ્યક્ષ પદ પરથી\n",
      "4: વધતા કેન્સર જેવા રોગો વધે છે તેમ ચર્ચામાં તજજ્ઞ અરૂણ દવેએ જણાવેલ હતું ગૃહમંત્રીએ કહ્યું કે મોદી સરકારે માં વાયદો\n",
      "5: જેમ ગાંધીનગર પાંચમો પોલીસ કમિશનરેટ વિસ્તાર બનશે જણાવી દઈએ કે આવી મહામારની કપરી સ્થીતીમાં મદદરૂપ બનવા પ્રજાએ જે પ્રતિનિધીઓને ચુંટયા\n"
     ]
    }
   ],
   "source": [
    "# Map n-gram names to their 'n'\n",
    "n_dict = {\n",
    "    \"unigram\": 1,\n",
    "    \"bigram\": 2,\n",
    "    \"trigram\": 3,\n",
    "    \"quadgram\": 4\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    n = n_dict.get(name)\n",
    "    if n is None:\n",
    "        print(f\"⚠️ Could not determine n for model '{name}', skipping\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== {name.upper()} MODEL (n={n}) ===\")\n",
    "\n",
    "    print(\"\\nGreedy Samples:\")\n",
    "    for i in range(5):  # set to 100 for full run\n",
    "        try:\n",
    "            print(f\"{i+1}: {greedy_generate(model, n)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error generating greedy sample: {e}\")\n",
    "\n",
    "    print(\"\\nBeam Search Samples:\")\n",
    "    for i in range(5):  # set to 100 for full run\n",
    "        try:\n",
    "            print(f\"{i+1}: {beam_search_generate(model, n)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error generating beam search sample: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b0427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model: unigram\n",
      "Model size: 1\n",
      "Processing model: bigram\n",
      "Model size: 8264\n",
      "Processing model: trigram\n",
      "Model size: 18821\n",
      "Processing model: quadgram\n",
      "Model size: 21237\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    print(f\"Processing model: {name}\")\n",
    "    print(f\"Model size: {len(model)}\")\n",
    "\n",
    "    n = {\"unigram\": 1, \"bigram\": 2}.get(name, 1)\n",
    "    ...\n",
    "   #hence problem with unigram model (left to solve yet) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
