{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fe0e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e57d0e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGE_STEPS = 32000\n",
    "VOCAB_SIZE = 32000\n",
    "SAVE_DIR = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d0b09d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ab17238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 1557 sentences.\n",
      "Total tokens for training: 21856\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "with open(\"tokenized.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "all_sentences = [sentence for sentence in data[\"sentences\"]]\n",
    "all_tokens = [token for sentence in data[\"tokens\"] for token in sentence]\n",
    "print(f\"Successfully loaded {len(all_sentences)} sentences.\")\n",
    "print(f\"Total tokens for training: {len(all_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_sentences_from_parquet(df):\n",
    "    sentences = []\n",
    "    for entry in df[\"sentences\"]:\n",
    "        # Case 1: numpy array of dicts\n",
    "        if isinstance(entry, np.ndarray):\n",
    "            for e in entry:\n",
    "                if isinstance(e, dict) and 'text' in e:\n",
    "                    text = e['text']\n",
    "                    tokens = text.strip().split()\n",
    "                    sentences.append(tokens)\n",
    "        # Case 2: single dict\n",
    "        elif isinstance(entry, dict) and 'text' in entry:\n",
    "            text = entry['text']\n",
    "            tokens = text.strip().split()\n",
    "            sentences.append(tokens)\n",
    "        # Case 3: list of dicts\n",
    "        elif isinstance(entry, list):\n",
    "            for e in entry:\n",
    "                if isinstance(e, dict) and 'text' in e:\n",
    "                    text = e['text']\n",
    "                    tokens = text.strip().split()\n",
    "                    sentences.append(tokens)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 161\n",
      "Most common: [(' ', 20351), ('ા', 11646), ('ર', 7269), ('ે', 7076), ('ન', 5874), ('્', 5255), ('ી', 5074), ('ક', 4233), ('મ', 4210), ('વ', 3934)]\n"
     ]
    }
   ],
   "source": [
    "word_freq = collections.Counter()\n",
    "for sent in all_sentences:\n",
    "    for word in sent:\n",
    "        word_freq[word] += 1\n",
    "\n",
    "print(f\"Unique words: {len(word_freq)}\")\n",
    "print(\"Most common:\", word_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a03a4a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocab size: 161\n"
     ]
    }
   ],
   "source": [
    "#prepare the initial vocabulary\n",
    "vocab = {' '.join(list(word)) + ' </w>': freq for word, freq in word_freq.items()}\n",
    "print(\"Initial vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1bc5e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    \"\"\"Count frequency of all adjacent symbol pairs in vocab.\"\"\"\n",
    "    pairs = collections.Counter()\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i+1])] += freq\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c439fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, v_in):\n",
    "    \"\"\"Merge the most frequent pair in vocab.\"\"\"\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    pattern = re.compile(r'(?<!\\\\S)' + bigram + r'(?!\\\\S)')\n",
    "    for word in v_in:\n",
    "        new_word = pattern.sub(''.join(pair), word)\n",
    "        v_out[new_word] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(MERGE_STEPS):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    if (i+1) % 1000 == 0:\n",
    "        print(f\"Step {i+1}: merged {best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ed98881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE vocabulary saved at bpe_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "bpe_vocab = set()\n",
    "for word in vocab:\n",
    "    bpe_vocab.update(word.split())\n",
    "\n",
    "with open(\"bpe_vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for token in sorted(bpe_vocab):\n",
    "        f.write(token + \"\\\\n\")\n",
    "\n",
    "print(\"BPE vocabulary saved at bpe_vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e74bb707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example BPE tokens: ['વી', 'ડિ', 'યો', '</w>']\n"
     ]
    }
   ],
   "source": [
    "def encode_bpe_word(word, merges):\n",
    "    word = list(word) + ['</w>']\n",
    "    pairs = [(word[i], word[i+1]) for i in range(len(word)-1)]\n",
    "    merge_set = set(merges)\n",
    "    while True:\n",
    "        bigrams = [(a,b) for (a,b) in pairs if (a,b) in merge_set]\n",
    "        if not bigrams:\n",
    "            break\n",
    "        a,b = bigrams[0]\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            if i < len(word)-1 and word[i]==a and word[i+1]==b:\n",
    "                new_word.append(a+b)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        word = new_word\n",
    "        if len(word) == 1:\n",
    "            break\n",
    "        pairs = [(word[i], word[i+1]) for i in range(len(word)-1)]\n",
    "    return word\n",
    "\n",
    "print(\"Example BPE tokens:\", encode_bpe_word(\"વીડિયો\", [(\"વ\",\"ી\"),(\"ડ\",\"િ\"),(\"ય\",\"ો\")])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5f58005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPiece vocab size: 161\n"
     ]
    }
   ],
   "source": [
    "def train_wordpiece(corpus, vocab_size):\n",
    "    vocab = collections.Counter()\n",
    "    for sent in corpus:\n",
    "        for word in sent:\n",
    "            vocab[word] += 1\n",
    "\n",
    "    # Start with all characters\n",
    "    wp_vocab = set()\n",
    "    for word in vocab:\n",
    "        wp_vocab.update(word)\n",
    "\n",
    "    wp_vocab = {ch: i for i, ch in enumerate(sorted(wp_vocab))}\n",
    "\n",
    "    while len(wp_vocab) < vocab_size:\n",
    "        pairs = collections.Counter()\n",
    "        for word, freq in vocab.items():\n",
    "            chars = list(word)\n",
    "            for i in range(len(chars) - 1):\n",
    "                pairs[(chars[i], chars[i+1])] += freq\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        new_token = best[0] + best[1]\n",
    "        wp_vocab[new_token] = len(wp_vocab)\n",
    "\n",
    "        new_vocab = collections.Counter()\n",
    "        for word, freq in vocab.items():\n",
    "            new_word = word.replace(new_token, new_token)\n",
    "            new_vocab[new_word] += freq\n",
    "        vocab = new_vocab\n",
    "\n",
    "    return wp_vocab\n",
    "\n",
    "wp_vocab = train_wordpiece(all_sentences, 32000)\n",
    "print(\"WordPiece vocab size:\", len(wp_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb075385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPiece vocabulary saved at wp_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"wp_vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for token in wp_vocab:\n",
    "        f.write(token + \"\\\\n\")\n",
    "\n",
    "print(\"WordPiece vocabulary saved at wp_vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6e5eb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example WordPiece tokens: ['વ', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "def encode_word_wordpiece(word, vocab):\n",
    "    output_tokens = []\n",
    "    start = 0\n",
    "    while start < len(word):\n",
    "        end = len(word)\n",
    "        sub = None\n",
    "        while start < end:\n",
    "            substr = word[start:end]\n",
    "            if start > 0:\n",
    "                substr = '##' + substr\n",
    "            if substr in vocab:\n",
    "                sub = substr\n",
    "                break\n",
    "            end -= 1\n",
    "        if sub is None:\n",
    "            output_tokens.append('[UNK]')\n",
    "            start += 1\n",
    "        else:\n",
    "            output_tokens.append(sub)\n",
    "            start = end\n",
    "    return output_tokens\n",
    "\n",
    "print(\"Example WordPiece tokens:\", encode_word_wordpiece(\"વીડિયો\", wp_vocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
